---
title: "Kaggle Titanic"
author: "bweigel"
date: "April 29, 2016"
output:
  knitrBootstrap::bootstrap_document:
    theme.chooser: TRUE
    highlight.chooser: TRUE
---

# So Kaggle ... #

This is my first attempt at Kaggle. I have registered some time ago, but never rwally got around to doing an y work. Mainly because I cannot really motivate to do something that seems more like a chore, than fun. Anyway, when I first looked at the Titanic dataset I though: "This should be straight-forward." And since I had done some basic data sciency stuff and I had also taken the course of Trevor Hastie and ? Tibshiriana I just couldn't motivate myself to actially start working.
So here we go.

```{r knitrOptions, cache=FALSE}
library(knitr)
opts_chunk$set(message=F, error=T, warning = F, echo=F, dev='CairoSVG')
```

```{r prerequesites}
library(dplyr)
library(magrittr)
library(pROC)
library(boot)
library(glmnet)
library(data.table)
library(ggplot2)
```

```{r load-dataset}
load(file="titanic.Rda")
```

## Create Fctor AgeGroup from previously established model ##

```{r AgeGroup}
ageLev <- c(0,1,3,10,15,25,50,100)
ageLab <- c("Infant", "Toddler", "Child", "Teenager", "Young Adult", "Mid-Aged", "Elderly")
# split Age into age-groups
df$AgeGroup <- cut(df$Age, breaks=ageLev, labels = ageLab, ordered_result = T)

dfAgeNA <- df[which(is.na(df$AgeGroup)),] %>% select(-Embarked, -Ticket, -Name)

# predict AgeGroup
```

The mortality of the passengers without an age is 10% higher than the average mortality (71% vs 62%). Also it seems that passengers w/o age were mostly in the lower classes 2 and 3 (avg class 2.6), male (70%) with rather cheap tickets (22 pounds) and were travelling without parents or children.

# Simple logistic regression #

At first I'll try a simple logistic regression and leave out any rows that contain `NA` values. Since the cabin column has 687 `NA` values this column was dropped before.

```{r}
#remove rows with NA values
dfWork <- df[!(is.na(df) %>% rowSums() %>% as.logical), ]

# split into train and test set
samples <- sample(nrow(dfWork), size = 0.6*nrow(dfWork))
dfWork_train <- dfWork[samples,]
dfWork_test <- dfWork[-samples,]

# perform simple logistic regression
logMod1 <- glm(Survived ~ ., family = binomial(link = "logit"), data = dfWork_train)
```

Unsurprisingly this simple approach doesn't work. The algorithm doesn't converge.
Let's fiddle around with the formula a bit.

```{r fitLogMod}
set.seed(1337)  
# simpler model
logMod1 <- glm(Survived ~ Pclass + Age + Sex + SibSp + Parch + Fare + SibSp:Age + SibSp:Pclass, family = binomial(link = "logit"), data = dfWork_train)

# seems like Parch and Fare are not significant as parameters for prediction
summary(logMod1)

# simplify model using step()
logMod2 <- step(logMod1)
summary(logMod2)
# as expected Parch and Fare are dropped
```

Now let's check the performance of our new model on te test set.

```{r predictLogMod}
# predict outcome
newY <- predict(logMod2, newdata = dfWork_test, type = "response")
dfAnalyze <- cbind(dfWork_test, SurvPred=newY) # this is for plotting later on
# compare prediction and actual values in confudion table
newY <- ifelse(newY <= 0.5, F, T)
confMat <- table(dfWork_test$Survived, newY)
knitr::kable(confMat)
confMat <- confMat/sum(confMat)
knitr::kable(confMat/sum(confMat))
```

This simple logistic regression model already does a reasonable well job of predicting the right outcome.
It classifies `r round((diag(confMat) %>% sum)*100,  2) `% of all cases into the right group.

```{r plotResultsLogMod}
ggplot(dfAnalyze %>% filter(SibSp == 0), aes(x = Age, y = SurvPred, color=Pclass, shape=Sex)) + 
  geom_point()  +
  geom_line() +
  labs(y="Predicted probability of Survival")
```


```{r glmCV}
logMod1 <- glm(Survived ~ Sex + Age + Embarked + Fare + Pclass + Parch + SibSp, dfWork, family= binomial(link="logit"))
logMod1 <- step(logMod1)
save(logMod1, file="models/logMod.Rda")
```

The 5-fold cross validation error or of the model `r print(logMod1$formula)` is approximately `r cv.glm(dfWork, logMod1, K=5)$delta[1]`.
