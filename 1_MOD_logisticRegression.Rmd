---
title: "Kaggle Titanic"
author: "bweigel"
date: "April 29, 2016"
output:
  knitrBootstrap::bootstrap_document:
    theme.chooser: TRUE
    highlight.chooser: TRUE
---
```{r knitrOptions, cache=FALSE}
library(knitr)
opts_chunk$set(message=F, error=T, warning = F, echo=F, dev='CairoSVG')
```

# Simple logistic regression #

At first I'll try a simple logistic regression and leave out any rows that contain `NA` values. Since the cabin column has 687 `NA` values this column was dropped before.

```{r Prerequesits}
library(dplyr)
library(magrittr)
library(boot)
library(glmnet)
library(data.table)
library(ggplot2)

load("titanic.Rda")
```



```{r}
set.seed(1337)  
#remove rows with NA values
dfWork <- df[!(is.na(df) %>% rowSums() %>% as.logical), ]

# split into train and test set
samples <- sample(nrow(dfWork), size = 0.6*nrow(dfWork))
dfWork_train <- dfWork[samples,]
dfWork_test <- dfWork[-samples,]

# perform simple logistic regression
logMod1 <- glm(Survived ~ ., family = binomial(link = "logit"), data = dfWork_train)
```

Unsurprisingly this simple approach doesn't work. The algorithm doesn't converge.
Let's fiddle around with the formula a bit.

```{r fitLogMod}
set.seed(1337)  
# simpler model
logMod1 <- glm(Survived ~ Pclass + Age + Sex + SibSp + Parch + Fare + SibSp:Age + SibSp:Pclass, family = binomial(link = "logit"), data = dfWork_train)

# seems like Parch and Fare are not significant as parameters for prediction
summary(logMod1)

# simplify model using step()
logMod2 <- step(logMod1)
summary(logMod2)
# as expected Parch and Fare are dropped

```

Now let's check the performance of our new model on te test set.

```{r predictLogMod}
# predict outcome
newY <- predict(logMod2, newdata = dfWork_test, type = "response")
dfAnalyze <- cbind(dfWork_test, SurvPred=newY) # this is for plotting later on
# compare prediction and actual values in confudion table
newY <- ifelse(newY <= 0.5, F, T)
confMat <- table(dfWork_test$Survived, newY)

knitr::kable(confMat)
confMat <- confMat/sum(confMat)
knitr::kable(confMat/sum(confMat))
```

This simple logistic regression model already does a reasonable well job of predicting the right outcome.
It classifies `r round((diag(confMat) %>% sum)*100,  2) `% of all cases into the right group.

```{r plotResultsLogMod}
ggplot(dfAnalyze %>% filter(SibSp == 0), aes(x = Age, y = SurvPred, color=Pclass, shape=Sex)) + 
  geom_point()  +
  geom_line() +
  labs(y="Predicted probability of Survival")
```


```{r glmCV}
logMod1 <- glm(Survived ~ Sex + Age + Embarked + Fare + Pclass + Parch + SibSp, dfWork, family= binomial(link="logit"))
logMod1 <- step(logMod1)
save(logMod1, file="models/logMod.Rda")
```

The 5-fold cross validation error or of the model `r print(logMod1$formula)` is approximately `r cv.glm(dfWork, logMod1, K=5)$delta[1]`.
